{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6979303,"sourceType":"datasetVersion","datasetId":4010627}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 3\n## Introduction\nIn this Assignment, we will implement a transformer model to finish a translation task between English and Chinese step by step. Before we start, you are suggested to read the original paper, the lecture of Hung-yi Lee and the blogs of the transformers. It may take much time to do those, however, only in this way, can you get the deep understanding of the task.\n\n### the original paper\n- paper: https://arxiv.org/pdf/1706.03762.pdf\n\n### transformer blog: \n- https://ketanhdoshi.github.io/Transformers-Overview/\n- https://ketanhdoshi.github.io/Transformers-Arch/\n- https://ketanhdoshi.github.io/Transformers-Attention/\n- https://ketanhdoshi.github.io/Transformers-Why/\n\n\n### the lecture of Hung-yi Lee\n\n- bilibili: https://www.bilibili.com/video/BV1v3411r78R/?spm_id_from=333.337.search-card.all.click&vd_source=155ff7fe8c811c0bd4176244f231e86b\n \n- slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf\n\nAlso, here are some implementations of the other frameworks which you can refer.\n\n- [implement of keras](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n- [implement of huggingface](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)\n## Download dataset\nLet's start with the dataset","metadata":{}},{"cell_type":"code","source":"# check your development environment.\nimport torch\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nimport logging\nimport sys\nfrom pathlib import Path\nimport datetime\nimport os\n\ndef beijing(sec, what):\n    beijing_time = datetime.datetime.now() + datetime.timedelta(hours=8)\n    return beijing_time.timetuple()\n\n\nlogging.Formatter.converter = beijing\n# set log\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s: %(message)s',\n                     datefmt='%Y-%m-%d %H:%M:%S',)\n\nlogging.info('The version information:')\nlogging.info(f'Python: {sys.version}')\nlogging.info(f'PyTorch: {torch.__version__}')\nassert torch.cuda.is_available() == True, 'Please finish your GPU develop environment'","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.648655Z","iopub.execute_input":"2023-11-16T08:47:44.649121Z","iopub.status.idle":"2023-11-16T08:47:44.658382Z","shell.execute_reply.started":"2023-11-16T08:47:44.649062Z","shell.execute_reply":"2023-11-16T08:47:44.657179Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Fix random seed","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\nseed = 2023\n\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  \nnp.random.seed(seed)  \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nlogging.info(f'The random seed is fixed to {seed}')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.660162Z","iopub.execute_input":"2023-11-16T08:47:44.660497Z","iopub.status.idle":"2023-11-16T08:47:44.672078Z","shell.execute_reply.started":"2023-11-16T08:47:44.660445Z","shell.execute_reply":"2023-11-16T08:47:44.671256Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Download and unzip files\nimport requests\n\n#define download function\ndef download(url, save_dir = Path.cwd()):\n    os.makedirs(save_dir, exist_ok = True)\n    \n    file_name = url.split('/')[-1]\n    file_path = save_dir / file_name\n    if file_path.exists():\n        logging.info(f'{file_name} exists!')\n        return \n    logging.info(f'downloading {file_name} from {url}')\n    response = requests.get(url)\n    if response.ok:\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n        logging.info(f\"download  {file_name} from {url} successfully!\")\n    else:\n        print(f\"Fail to download  {file_name} from {url}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.673115Z","iopub.execute_input":"2023-11-16T08:47:44.673390Z","iopub.status.idle":"2023-11-16T08:47:44.684178Z","shell.execute_reply.started":"2023-11-16T08:47:44.673366Z","shell.execute_reply":"2023-11-16T08:47:44.683356Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"dataset_url = 'http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz'","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.686289Z","iopub.execute_input":"2023-11-16T08:47:44.686574Z","iopub.status.idle":"2023-11-16T08:47:44.697621Z","shell.execute_reply.started":"2023-11-16T08:47:44.686548Z","shell.execute_reply":"2023-11-16T08:47:44.696765Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"dataset_dir = Path('dataset')\ndownload(dataset_url, save_dir=dataset_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.699081Z","iopub.execute_input":"2023-11-16T08:47:44.699420Z","iopub.status.idle":"2023-11-16T08:47:44.708724Z","shell.execute_reply.started":"2023-11-16T08:47:44.699393Z","shell.execute_reply":"2023-11-16T08:47:44.707809Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import tarfile\ntgz_file_path = dataset_dir / 'training-parallel-nc-v13.tgz'\ndataset_path = dataset_dir / 'training-parallel-nc-v13'","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.710197Z","iopub.execute_input":"2023-11-16T08:47:44.710541Z","iopub.status.idle":"2023-11-16T08:47:44.719080Z","shell.execute_reply.started":"2023-11-16T08:47:44.710508Z","shell.execute_reply":"2023-11-16T08:47:44.718128Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"tar = tarfile.open(tgz_file_path)\nif not dataset_path.exists():\n    logging.info(f\"exact {tgz_file_path} to {dataset_dir}\")\n    tar.extractall(dataset_dir)\nelse:\n    logging.info(f\"{dataset_path} exists!\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.720247Z","iopub.execute_input":"2023-11-16T08:47:44.720539Z","iopub.status.idle":"2023-11-16T08:47:44.729179Z","shell.execute_reply.started":"2023-11-16T08:47:44.720505Z","shell.execute_reply":"2023-11-16T08:47:44.728355Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"chinese_data_path = dataset_path / 'news-commentary-v13.zh-en.zh'\nenglish_data_path = dataset_path / 'news-commentary-v13.zh-en.en'","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.730331Z","iopub.execute_input":"2023-11-16T08:47:44.730675Z","iopub.status.idle":"2023-11-16T08:47:44.738511Z","shell.execute_reply.started":"2023-11-16T08:47:44.730647Z","shell.execute_reply":"2023-11-16T08:47:44.737530Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Get corpus","metadata":{}},{"cell_type":"code","source":"chinese_lines = []\nenglish_lines = []","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.740223Z","iopub.execute_input":"2023-11-16T08:47:44.740650Z","iopub.status.idle":"2023-11-16T08:47:44.747975Z","shell.execute_reply.started":"2023-11-16T08:47:44.740581Z","shell.execute_reply":"2023-11-16T08:47:44.747053Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"chinese_data_file = open(chinese_data_path, 'r')\nenglish_data_file = open(english_data_path, 'r')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.751361Z","iopub.execute_input":"2023-11-16T08:47:44.751644Z","iopub.status.idle":"2023-11-16T08:47:44.760040Z","shell.execute_reply.started":"2023-11-16T08:47:44.751619Z","shell.execute_reply":"2023-11-16T08:47:44.759119Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"chinese_data_list = list(chinese_data_file.readlines())\nenglish_data_list = list(english_data_file.readlines())","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:44.760872Z","iopub.execute_input":"2023-11-16T08:47:44.761246Z","iopub.status.idle":"2023-11-16T08:47:45.087380Z","shell.execute_reply.started":"2023-11-16T08:47:44.761221Z","shell.execute_reply":"2023-11-16T08:47:45.086560Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"assert  len(chinese_data_list) == len(english_data_list) and len(chinese_data_list) == 252777, \\\n    'The number of sample error! Please load the dataset again'","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:45.088505Z","iopub.execute_input":"2023-11-16T08:47:45.088812Z","iopub.status.idle":"2023-11-16T08:47:45.093614Z","shell.execute_reply.started":"2023-11-16T08:47:45.088786Z","shell.execute_reply":"2023-11-16T08:47:45.092648Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"number_of_samples = 5\nindex = 0\nfor chinese_sentence, english_sentence in zip(chinese_data_list, english_data_list):\n    print(index, '\\n Chinese sentence: ' + chinese_sentence, 'English sentence: ' , english_sentence)\n    index = index + 1\n    if index > number_of_samples:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:45.094819Z","iopub.execute_input":"2023-11-16T08:47:45.095147Z","iopub.status.idle":"2023-11-16T08:47:45.103971Z","shell.execute_reply.started":"2023-11-16T08:47:45.095110Z","shell.execute_reply":"2023-11-16T08:47:45.103133Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"0 \n Chinese sentence: 1929年还是1989年?\n English sentence:  1929 or 1989?\n\n1 \n Chinese sentence: 巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n English sentence:  PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n\n2 \n Chinese sentence: 一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。\n English sentence:  At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.\n\n3 \n Chinese sentence: 如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。\n English sentence:  Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.\n\n4 \n Chinese sentence: 目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。\n English sentence:  The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).\n\n5 \n Chinese sentence: 欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时机来实行急需的结构性改革。\n English sentence:  Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset division","metadata":{}},{"cell_type":"code","source":"dataset_list = []\nfor chinese_sentence, english_sentence in zip(chinese_data_list, english_data_list):\n    dataset_list.append([english_sentence.replace('\\n',''), chinese_sentence.replace('\\n','')])\nprint(dataset_list[:5])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:45.104900Z","iopub.execute_input":"2023-11-16T08:47:45.105175Z","iopub.status.idle":"2023-11-16T08:47:46.183770Z","shell.execute_reply.started":"2023-11-16T08:47:45.105152Z","shell.execute_reply":"2023-11-16T08:47:46.182552Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"[['1929 or 1989?', '1929年还是1989年?'], ['PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.', '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。'], ['At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.', '一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。'], ['Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.', '如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。'], ['The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).', '目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。']]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# train:test:dev = 8:1:1\ntrain_dataset, test_and_dev_dataset = train_test_split(dataset_list, shuffle=True, test_size=0.2, random_state=2023)\ntest_dataset, dev_dataset = train_test_split(test_and_dev_dataset, shuffle=True, test_size=0.5, random_state=2023)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:46.185196Z","iopub.execute_input":"2023-11-16T08:47:46.185550Z","iopub.status.idle":"2023-11-16T08:47:46.435090Z","shell.execute_reply.started":"2023-11-16T08:47:46.185517Z","shell.execute_reply":"2023-11-16T08:47:46.433341Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train_dataset[:5]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:46.437193Z","iopub.execute_input":"2023-11-16T08:47:46.437511Z","iopub.status.idle":"2023-11-16T08:47:46.445327Z","shell.execute_reply.started":"2023-11-16T08:47:46.437483Z","shell.execute_reply":"2023-11-16T08:47:46.444393Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[['ABU DHABI – In Hermann Hesse’s novel Journey to the East, the character of H.H., a novice in a religious group known as The League, describes a figurine depicting himself next to the group’s leader, Leo.',\n  '阿布扎比—赫尔曼·黑塞（Hermann Hesse）的小说《东游记》（Journey to the East）中的角色H. H.'],\n ['The second critical aspect is how one treats future outcomes relative to current ones – an issue that has aroused much attention among philosophers as well as economists.',\n  '第二个关键的方面是人们应该看待未来的结果与当前的人的关系，这个问题在哲学家以及经济学家都引起了很大的关注。'],\n ['Pioneering Moroccan feminists began their work soon after independence in 1956.',\n  '摩洛哥女权主义先驱们在1956年国家获得独立之后不久就投身于他们的追求。'],\n ['Stopping the spread of nuclear weapons, promoting more efficient energy use, taking action on climate change, and maintaining an open global economy – these and other tasks require Chinese participation, even cooperation, if globalization is not to overwhelm us all.',\n  '阻止核武器扩散，促进能源更有效地利用，对气候变化采取措施以及维护开放的全球经济——这些以及其它的工作都需要中国的参与，甚至是合作，如果我们不想全球化压倒我们的话。'],\n ['The gullible Sancho Panza was meant to adopt the revolution’s deceptive dogma as entitlement to wage a brutal war against all.',\n  '容易上当受骗的桑丘·潘沙定会接受革命欺骗性的信条主张，把它作为发动一场反对所有人的战争依据。']]"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset[:5]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:46.446561Z","iopub.execute_input":"2023-11-16T08:47:46.446949Z","iopub.status.idle":"2023-11-16T08:47:46.460988Z","shell.execute_reply.started":"2023-11-16T08:47:46.446893Z","shell.execute_reply":"2023-11-16T08:47:46.460058Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"[['Electrifying agricultural areas would facilitate the storage and transportation of farmed products, improve food security, and increase farmers’ earning capacity.',\n  '电气化农业区能够便利农作物的储存和 运输，改善粮食安全，提高农民的收入能力。'],\n ['Why, then, does the narrative still have such a hold on us today?',\n  '那么，为何这一叙事至今仍有如此大的市场？'],\n ['Ideology trumps factuality.', '意识形态凌驾于事实。'],\n ['The tax would make debt more expensive, because it would be taxed, while making equity cheaper, because profits would not be taxed.',\n  '税收将让债务变得更贵，因为债务将被课税，同时股本会变得更便宜，因为利润不会被课税。'],\n ['Governments will become more effective in the future only if voters learn to become more demanding of the policies that future governments adopt.',\n  '如果选民学会提高对未来政府的政策要求，政府或许能在不久的将来实现效率提升。']]"},"metadata":{}}]},{"cell_type":"code","source":"dev_dataset[:5]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:46.462096Z","iopub.execute_input":"2023-11-16T08:47:46.462397Z","iopub.status.idle":"2023-11-16T08:47:46.471027Z","shell.execute_reply.started":"2023-11-16T08:47:46.462368Z","shell.execute_reply":"2023-11-16T08:47:46.470040Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"[['Even the most conservative estimates paint a grim picture.',\n  '即使是最保守的估计也相当惨淡。'],\n ['Ethical machines would pose no threat to humanity.', '有伦理的机器不会造成人道威胁。'],\n ['The classic case is Britain’s short-lived return to gold in the interwar period.',\n  '两次世界大战期间英国曾短暂恢复金本位制是这方面的经典案例。'],\n ['Because it is.', '因为事实就是如此。'],\n ['But this interpretation is a complete – and sometimes deliberate – misunderstanding of bank capital and the policies being pursued.',\n  '但这一解读完全是对银行资本以及目前所追求的政策的误解，有时这是有意为之。']]"},"metadata":{}}]},{"cell_type":"code","source":"len(train_dataset), len(test_dataset), len(dev_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:46.472382Z","iopub.execute_input":"2023-11-16T08:47:46.472834Z","iopub.status.idle":"2023-11-16T08:47:46.482704Z","shell.execute_reply.started":"2023-11-16T08:47:46.472800Z","shell.execute_reply":"2023-11-16T08:47:46.481639Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(202221, 25278, 25278)"},"metadata":{}}]},{"cell_type":"code","source":"import json\njson_dir_path = Path('dataset')\nPath.mkdir(json_dir_path, exist_ok=True)\n\ndataset_list = [train_dataset, test_dataset, dev_dataset]\njson_name_list = ['train.json', 'test.json', 'dev.json']\nfor dataset, json_name in zip(dataset_list, json_name_list):\n    dataset_path = json_dir_path / json_name\n    json_data = json.dumps(dataset)\n    with open(dataset_path, \"w\",encoding = 'utf-8') as file:\n        file.write(json_data)\n    print(f'save {json_name} to {dataset_path}')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:46.489893Z","iopub.execute_input":"2023-11-16T08:47:46.490230Z","iopub.status.idle":"2023-11-16T08:47:47.401715Z","shell.execute_reply.started":"2023-11-16T08:47:46.490200Z","shell.execute_reply":"2023-11-16T08:47:47.400348Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"save train.json to dataset/train.json\nsave test.json to dataset/test.json\nsave dev.json to dataset/dev.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenize\nThe step of Tokenizing is a very important step in the natrual language process(NLP) field. Please refer to [blog:why and how to tokenize](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt) to learn more about this process. \n[SentencePiece](https://github.com/google/sentencepiece/) is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\nIn this assignment, we will use the Byte-Pair Encoding([BPE](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)) tokenization. Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa.","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm\ndef train(input_file, vocab_size, model_name, model_type, character_coverage):\n    \"\"\"\n    search on https://github.com/google/sentencepiece/blob/master/doc/options.md to learn more about the parameters\n    :param input_file: one-sentence-per-line raw corpus file. No need to run tokenizer, normalizer or preprocessor.\n                       By default, SentencePiece normalizes the input with Unicode NFKC.\n                       You can pass a comma-separated list of files.\n    :param vocab_size: vocabulary size, e.g., 8000, 16000, or 32000\n    :param model_name: output model name prefix. <model_name>.model and <model_name>.vocab are generated.\n    :param model_type: model type. Choose from unigram (default), bpe, char, or word.\n                       The input sentence must be pretokenized when using word type.\n    :param character_coverage: amount of characters covered by the model, good defaults are: 0.9995 for languages with\n                               rich character set like Japanse or Chinese and 1.0 for other languages with\n                               small character set.\n    \"\"\"\n    input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s ' \\\n                     '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '\n    cmd = input_argument % (input_file, model_name, vocab_size, model_type, character_coverage)\n    cmd = 'spm_train '+ cmd\n    # spm.SentencePieceTrainer.Train(cmd)\n    # in this assignment, we use the following command\n    os.system(cmd)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.403327Z","iopub.execute_input":"2023-11-16T08:47:47.403812Z","iopub.status.idle":"2023-11-16T08:47:47.411285Z","shell.execute_reply.started":"2023-11-16T08:47:47.403763Z","shell.execute_reply":"2023-11-16T08:47:47.410229Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"tokenizer_dir  = Path('tokenizer')\nos.makedirs(tokenizer_dir, exist_ok = True)\neng_model_path = tokenizer_dir / Path('eng.model')\neng_vocab_path = tokenizer_dir /Path('eng.vocab')\nchn_model_path = tokenizer_dir /Path('chn.model')\nchn_vocab_path = tokenizer_dir / Path('chn.vocab')\neng_model_path.exists(),eng_vocab_path.exists(),chn_model_path.exists(), chn_vocab_path.exists()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.412527Z","iopub.execute_input":"2023-11-16T08:47:47.412857Z","iopub.status.idle":"2023-11-16T08:47:47.427488Z","shell.execute_reply.started":"2023-11-16T08:47:47.412829Z","shell.execute_reply":"2023-11-16T08:47:47.426396Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"(True, True, True, True)"},"metadata":{}}]},{"cell_type":"code","source":"# To fix the problem of \"spm_train: not found\" in kaggle, I add these statement.\n# refer to: https://stackoverflow.com/questions/55278519/using-sentencepiece-as-a-command\n# % git clone https://github.com/google/sentencepiece.git \n# % cd sentencepiece\n# % mkdir build\n# % cd build\n# % cmake ..\n# % make -j $(nproc)\n# % sudo make install\n# % sudo ldconfig -v","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.428983Z","iopub.execute_input":"2023-11-16T08:47:47.429533Z","iopub.status.idle":"2023-11-16T08:47:47.435764Z","shell.execute_reply.started":"2023-11-16T08:47:47.429494Z","shell.execute_reply":"2023-11-16T08:47:47.434950Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"en_input = english_data_path\nen_vocab_size = 32000\nen_model_name = tokenizer_dir / Path('eng')\nen_model_type = 'bpe'\nen_character_coverage = 1\n\ntokenizer_dir  = Path('tokenizer')\neng_model_path = tokenizer_dir / Path('eng.model')\neng_vocab_path = tokenizer_dir /Path('eng.vocab')\n\nif eng_model_path.exists() and eng_vocab_path.exists():\n    logging.info(f\"{eng_model_path } and {eng_vocab_path} have exist! continue run the code\")\nelse:\n    train(en_input, en_vocab_size, en_model_name, en_model_type, en_character_coverage)\n\nch_input = chinese_data_path\nch_vocab_size = 32000\nch_model_name = tokenizer_dir / Path('chn')\nch_model_type = 'bpe'\nch_character_coverage = 0.9995\n\nchn_model_path = tokenizer_dir / Path('chn.model')\nchn_vocab_path = tokenizer_dir / Path('chn.vocab')\nif chn_model_path.exists() and chn_vocab_path.exists():\n    logging.info(f\"{chn_model_path } and {chn_vocab_path} have exist! continue run the code\")\nelse:\n    train(ch_input, ch_vocab_size, ch_model_name, ch_model_type, ch_character_coverage)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.437031Z","iopub.execute_input":"2023-11-16T08:47:47.437386Z","iopub.status.idle":"2023-11-16T08:47:47.451554Z","shell.execute_reply.started":"2023-11-16T08:47:47.437353Z","shell.execute_reply":"2023-11-16T08:47:47.450746Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## SentencePiece test\nAfter we finish the sentencepiece training, let's do some test to get the understanding of its work processing","metadata":{}},{"cell_type":"code","source":"sp = spm.SentencePieceProcessor()\ntext = \"美国总统特朗普今日抵达夏威夷。\"\n\nsp.Load('./tokenizer/chn.model')\nprint(sp.EncodeAsPieces(text))\n\n# encode the text\ns =sp.EncodeAsIds(text)\n# embeding vector\nprint(s)\n# decode the embedding vector\nprint(sp.decode_ids(s))\n\n# let's do little change to the embeding functio vector\nfor i in range(0,len(s),2):\n    print(f'{i}: {s[i]} --> {s[i] + 1}')\n    s[i] = s[i] + 1 \n# look new vector\nprint(s)\n# decode the new embedding vector\nprint(sp.decode_ids(s))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.452665Z","iopub.execute_input":"2023-11-16T08:47:47.453005Z","iopub.status.idle":"2023-11-16T08:47:47.483146Z","shell.execute_reply.started":"2023-11-16T08:47:47.452974Z","shell.execute_reply":"2023-11-16T08:47:47.482181Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"['▁美国总统', '特朗普', '今日', '抵达', '夏威夷', '。']\n[12908, 277, 7420, 7319, 18385, 28724]\n美国总统特朗普今日抵达夏威夷。\n0: 12908 --> 12909\n2: 7420 --> 7421\n4: 18385 --> 18386\n[12909, 277, 7421, 7319, 18386, 28724]\n传染性疾病特朗普减记抵达学生们。\n","output_type":"stream"}]},{"cell_type":"code","source":"sp = spm.SentencePieceProcessor()\ntext = \"U.S. President Donald Trump arrived in Hawaii today.\"\n\n# do same as above, but English\nsp.Load('./tokenizer/eng.model')\nprint(sp.EncodeAsPieces(text))\ns =sp.EncodeAsIds(text)\nprint(s)\nprint(sp.decode_ids(s))\n\nfor i in range(0,len(s),2):\n    print(f'{i}: {s[i]} --> {s[i] + 1}')\n    s[i] = s[i] + 1 \nprint(s)\nprint(sp.decode_ids(s))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.484424Z","iopub.execute_input":"2023-11-16T08:47:47.484785Z","iopub.status.idle":"2023-11-16T08:47:47.517296Z","shell.execute_reply.started":"2023-11-16T08:47:47.484748Z","shell.execute_reply":"2023-11-16T08:47:47.516290Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"['▁U', '.', 'S', '.', '▁President', '▁Donald', '▁Trump', '▁arrived', '▁in', '▁Hawaii', '▁today', '.']\n[131, 31843, 31850, 31843, 811, 3575, 1023, 8437, 26, 18096, 858, 31843]\nU.S. President Donald Trump arrived in Hawaii today.\n0: 131 --> 132\n2: 31850 --> 31851\n4: 811 --> 812\n6: 1023 --> 1024\n8: 26 --> 27\n10: 858 --> 859\n[132, 31843, 31851, 31843, 812, 3575, 1024, 8437, 27, 18096, 859, 31843]\nres.x. foreign Donald consum arriveded Hawaii future.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Config\nHere is the configuration of this assignment, you are only allowed to change these parameters:\n- batch_size\n- epoch_num\n- lr\n- beam size\n- gpu_id\n- device_id","metadata":{}},{"cell_type":"code","source":"from argparse import Namespace\nimport torch\n\ndataset_path = Path('dataset')\nexperiment_path = Path('experiment')\nPath.mkdir(dataset_path, exist_ok=True)\nPath.mkdir(experiment_path, exist_ok=True)\n\nconfig = Namespace(\nd_model = 512,\nn_heads = 8,\nn_layers = 6,\nd_k = 64,\nd_v = 64,\nd_ff = 2048,\ndropout = 0.1,\npadding_idx = 0,\nbos_idx = 2,\neos_idx = 3,\nsrc_vocab_size = 32000,\ntgt_vocab_size = 32000,\nbatch_size = 128,\nepoch_num = 100,\nearly_stop = 5,\nlr = 3e-4,\n\n# the max length of sentence in greed decode\nmax_len = 60,\n# beam size for bleu\nbeam_size = 3,\n# Label Smoothing\nuse_smoothing = False,\n# NoamOpt\nuse_noamopt = True,\n\ntrain_data_path = dataset_path / 'train.json',\ndev_data_path = dataset_path / 'dev.json',\ntest_data_path = dataset_path / 'test.json',\noutput_model_path = experiment_path / 'model.pth',\nlog_path = experiment_path / 'train.log',\noutput_path = experiment_path / 'output.txt',\n\n# gpu_id and device id is the relative id\n# thus, if you wanna use os.environ['CUDA_VISIBLE_DEVICES'] = '2, 3'\n# you should set CUDA_VISIBLE_DEVICES = 2 as main -> gpu_id = '0', device_id = [0, 1]\ngpu_id = '0',\ndevice_id = [0, 1],\n)\n# set device\nif config.gpu_id != '':\n    device = torch.device(f\"cuda:{config.gpu_id}\")\nelse:\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.518496Z","iopub.execute_input":"2023-11-16T08:47:47.518794Z","iopub.status.idle":"2023-11-16T08:47:47.541789Z","shell.execute_reply.started":"2023-11-16T08:47:47.518756Z","shell.execute_reply":"2023-11-16T08:47:47.540759Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.543001Z","iopub.execute_input":"2023-11-16T08:47:47.543290Z","iopub.status.idle":"2023-11-16T08:47:47.554510Z","shell.execute_reply.started":"2023-11-16T08:47:47.543264Z","shell.execute_reply":"2023-11-16T08:47:47.553602Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm\n\n\ndef chinese_tokenizer_load():\n    sp_chn = spm.SentencePieceProcessor()\n    sp_chn.Load('./tokenizer/chn.model')\n    return sp_chn\n\n\ndef english_tokenizer_load():\n    sp_eng = spm.SentencePieceProcessor()\n    sp_eng.Load('./tokenizer/eng.model')\n    return sp_eng\n\ndef set_logger(log_path):\n    \"\"\"Set the logger to log info in terminal and file `log_path`.\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n    Example:\n    ```\n    logging.info(\"Starting training...\")\n    ```\n    Args:\n        log_path: (string) where to log\n    \"\"\"\n    if os.path.exists(log_path) is True:\n        os.remove(log_path)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n        logger.addHandler(stream_handler)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.555667Z","iopub.execute_input":"2023-11-16T08:47:47.555964Z","iopub.status.idle":"2023-11-16T08:47:47.564611Z","shell.execute_reply.started":"2023-11-16T08:47:47.555932Z","shell.execute_reply":"2023-11-16T08:47:47.563849Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## Finish the Class: MTDataset(10 marks)\nYou are supposed to finish the code of the class **MTDataset** in following block.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nDEVICE = device\n\n\ndef subsequent_mask(size): \n    \"\"\"Mask out subsequent positions.\"\"\"\n    # set the shape of subsequent_mask matrix\n    attn_shape = (1, size, size)\n\n    # create a subsequent_mask matrix with ones in the upper right corner (excluding the main diagonal) and zeros in the lower left corner (including the main diagonal).\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n\n    # return a subsequent_mask matrix with False in the upper right corner (excluding the main diagonal) and True in the lower left corner (including the main diagonal).\n    return torch.from_numpy(subsequent_mask) == 0\n\n\nclass Batch:\n    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n    def __init__(self, src_text, trg_text, src, trg=None, pad=0):\n        self.src_text = src_text\n        self.trg_text = trg_text\n        src = src.to(DEVICE)\n        self.src = src\n        # Determine the non-empty part of the current input sentence as a bool sequence.\n        # And add one dimension in front of seq length to form a matrix of dimension 1×seq length\n        self.src_mask = (src != pad).unsqueeze(-2)\n        # If the output target is not null, then you need to mask the target clause to be used by the decoder.\n        if trg is not None:\n            trg = trg.to(DEVICE)\n            # Target input part to be used by decoder\n            self.trg = trg[:, :-1]\n            # The decoder training should predict the output target result\n            self.trg_y = trg[:, 1:]\n            # Attention mask the target input portion\n            self.trg_mask = self.make_std_mask(self.trg, pad)\n            # Counts the actual number of words in the target results that should be outputted\n            self.ntokens = (self.trg_y != pad).data.sum()\n\n    # Mask\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        \"\"\"Create a mask to hide padding and future words.\"\"\"\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n        return tgt_mask\n\n\nclass MTDataset(Dataset):\n    def __init__(self, data_path):\n        self.out_en_sent, self.out_cn_sent = self.get_dataset(data_path, sort=True)\n        self.sp_eng = english_tokenizer_load()\n        self.sp_chn = chinese_tokenizer_load()\n        self.PAD = self.sp_eng.pad_id()  # 0\n        self.BOS = self.sp_eng.bos_id()  # 2\n        self.EOS = self.sp_eng.eos_id()  # 3\n\n    @staticmethod\n    def len_argsort(seq):\n        \"\"\"\n        Input: A list of tokenized sentences.\n        Output: Indices that would sort the sentences by their lengths.\n\n        This static method takes in a list of tokenized sentences and returns \n        a list of indices that would sort the sentences by their lengths.\n        \"\"\"\n        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n\n    def get_dataset(self, data_path, sort=False):\n        \"\"\"Sort Chinese and English in the same order, using the English sentence length ordering (sentence subscripts) as the base.\"\"\"\n        logging.info(f\"get_dataset from:{os.path.abspath(data_path)}\")\n        dataset = json.load(open(data_path, 'r'))\n        out_en_sent = []\n        out_cn_sent = []\n        for idx, _ in enumerate(dataset):\n            out_en_sent.append(dataset[idx][0])\n            out_cn_sent.append(dataset[idx][1])\n        if sort:\n            sorted_index = self.len_argsort(out_en_sent)\n            out_en_sent = [out_en_sent[i] for i in sorted_index]\n            out_cn_sent = [out_cn_sent[i] for i in sorted_index]\n        return out_en_sent, out_cn_sent\n\n    def __getitem__(self, idx):\n        eng_text = self.out_en_sent[idx]\n        chn_text = self.out_cn_sent[idx]\n        return [eng_text, chn_text]\n\n    def __len__(self):\n        return len(self.out_en_sent)\n\n    def collate_fn(self, batch):\n        \"\"\"\n        Input: A batch of data.\n        Output: A Batch object containing source and target texts, and their tokenized and padded versions.\n\n        This method is responsible for:\n        1. Extracting English and Chinese texts from the batch.\n        2. Tokenizing and padding these texts.\n        3. Returning a Batch object that holds these details.\n        \"\"\"\n        src_text = [x[0] for x in batch]\n        tgt_text = [x[1] for x in batch]\n        \n        src = [self.sp_eng.EncodeAsIds(text) for text in src_text]\n        src = [torch.tensor(x) for x in src]\n        \n        tgt = [self.sp_chn.EncodeAsIds(text) for text in tgt_text]\n        tgt = [torch.tensor(x) for x in tgt]\n\n        src = pad_sequence(src, padding_value=self.PAD)\n        tgt = pad_sequence(tgt, padding_value=self.PAD)\n\n        return Batch(src_text, tgt_text, src, tgt, self.PAD)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.565831Z","iopub.execute_input":"2023-11-16T08:47:47.566122Z","iopub.status.idle":"2023-11-16T08:47:47.588579Z","shell.execute_reply.started":"2023-11-16T08:47:47.566098Z","shell.execute_reply":"2023-11-16T08:47:47.587779Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"## Finish the implement of Label Smoothing, Embeddings and Softmax, Positional Encoding, Attention and Position-wise Feed-Forward Networks(20 marks)\n### Label Smoothing\nLabel Smoothing hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\nWe implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary\n\n### Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension$d_model$.\nWe also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (cite)[https://arxiv.org/abs/1608.05859]. In the embedding layers, we multiply those weights by $\\sqrt d_{model}$\n\n### Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension$\\sqrt d_{model}$. as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (cite)[https://arxiv.org/pdf/1705.03122.pdf]\nIn this work, we use sine and cosine functions of different frequencies\n$$\n\\begin{equation*}\nPE_{pos, 2i} = sin(pos/10000^{2i/d_{model}}) \n\\end{equation*}\n$$\n$$\n\\begin{equation*}\nPE_{pos, 2i+1} = cos(pos/10000^{2i/d_{model}}) \n\\end{equation*}\n$$\nwhere $pos$ is the position and $i$ is the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $1000 * 2\\pi$. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos + k}$ can be represented as a linear function of $PE_{pos}$.\n\n\nIn addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop} = 0.1$\n\n### Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\n### Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.","metadata":{}},{"cell_type":"code","source":"import math\nimport copy\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"Implement label smoothing.\"\"\"\n\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False)\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n\n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, Variable(true_dist, requires_grad=False))\n\n\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        # Embedding layer\n        self.lut = nn.Embedding(vocab, d_model)\n        # Embedding dimension \n        self.d_model = d_model\n\n    def forward(self, x):\n        \"\"\"\n        Input: Tensor 'x' containing token indices.\n        Output: The corresponding embedding matrix, scaled by the square root of the embedding dimension.\n        \n        Fetches the embeddings for the given token indices and scales the result by \n        the square root of the embedding dimension.\n        \"\"\"\n        return self.lut(x) * math.sqrt(self.d_model)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Initialize an all-zero matrix with a size of max_len (the set maximum length) × embedding dimension.\n        # To hold the positional embedding of all positions less than this length.\n        pe = torch.zeros(max_len, d_model, device=DEVICE)\n        # Generate a position subscripted tensor matrix (each row is a position subscript)\n        \"\"\"\n        Forms like:\n        tensor([[0.],\n                [1.],\n                [2.],\n                [3.],\n                [4.],\n                ...])\n        \"\"\"\n        position = torch.arange(0., max_len, device=DEVICE).unsqueeze(1)\n        # Here the power operation is too much, we use exp and log to convert the denominator to be divided below the pos in the realization formula \n        # Be careful with the negative sign since it is the denominator\n        div_term = torch.exp(torch.arange(0., d_model, 2, device=DEVICE) * -(math.log(10000.0) / d_model))\n\n        # According to the formula, the positional texture values of each position in each embedding dimension are calculated and stored into the pe matrix\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # Add 1 dimension so that the pe dimension becomes: 1 x max_len x embedding dimension\n        # (to facilitate subsequent batch summing of embedding of all words of a sentence with a batch)\n        pe = pe.unsqueeze(0)\n        # Save the pe matrix in a persistent buffer state (will not be used as a parameter to be trained)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Input: Tensor 'x' containing embeddings.\n        Output: Tensor with positional encodings added to the embeddings.\n        \n        Process:\n        1. Add positional encodings to the given embeddings 'x'.\n        2. Ensure the positional encodings are aligned with the sequence length of 'x'.\n        3. Apply dropout (defined in __init__) before returning.\n        \"\"\"\n        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n        return self.dropout(x)\n\n\ndef attention(query, key, value, mask=None, dropout=None):\n    \"\"\"\n    Compute the scaled dot product attention.\n    \n    Input:\n    - query, key, value: Tensors for the query, key, and value\n    - mask: Optional tensor to mask certain values \n    - dropout: Optional dropout layer for regularization\n    \n    Steps:\n    1. Calculate 'scores' by computing the dot product of 'query' and 'key'. Don't forget to scale it.\n    2. If a mask is provided, apply it to the 'scores' tensor. The idea is to set masked positions to a large negative value.\n    3. Apply softmax to the 'scores' to get the attention probabilities.\n    4. If a dropout layer is provided, apply dropout to the attention probabilities.\n    5. Finally, compute the output by multiplying the attention probabilities with 'value'.\n    \n    Returns:\n    - The result tensor and the attention probabilities.\n    \"\"\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    print(\"==================================================================\")\n    print(query.size())\n    print(key.size())\n    print(value.size())\n    print(mask.size()) \n    print(scores.size())\n    print(\"==================================================================\")\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\n\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        # Guaranteed to be divisible\n        assert d_model % h == 0\n        # Get a HEAD's ATTENTION representation of the dimension\n        self.d_k = d_model // h\n        # Number of heads\n        self.h = h\n        # Define 4 fully connected functions for subsequent use as the WQ, WK, WV matrices and the last h polytopic attention matrices to be transformed after concat\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Forward propagation for multi-headed attention.\n        \n        Input:\n        - query, key, value: Tensors for query, key, and value.\n        - mask: Optional tensor to mask certain values.\n        \n        Steps:\n        1. If mask is provided, adjust its shape.\n        2. Find the batch size from the 'query' tensor.\n        3. Apply the WQ, WK, WV transformations to query, key, and value respectively.\n        4. Split the transformed tensors into 'h' blocks.\n        5. For each block, calculate the attention values.\n        6. Concatenate all the attention blocks.\n        7. Apply the final linear transformation.\n        \n        Returns:\n        - The output tensor after multi-headed attention.\n        \n        Note: You might want to revisit the 'attention' function you implemented before.\n        \"\"\"\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            \n        # 保持query, key, value的大小不变        \n        query = self.linears[0](query)\n        key = self.linears[1](key) \n        value = self.linears[2](value)\n\n        nbatches = query.size(0)\n\n        query = query.view(nbatches, -1, self.h, self.d_k).transpose(1, 2) \n        key = key.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n        value = value.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n\n        x, self.attn = attention(query, key, value, mask=mask,                 \n                                 dropout=self.dropout)\n\n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d_k)\n        \n        return self.linears[-1](x)\n\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        # Initialize α to all 1's and β to all 0's.\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        # smooth term (in calculus)\n        self.eps = eps\n\n    def forward(self, x):\n        # Calculate mean and variance by last dimension\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n\n        # Returns the result of Layer Norm\n        return self.a_2 * (x - mean) / torch.sqrt(std ** 2 + self.eps) + self.b_2\n\n\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    The role of SublayerConnection is to connect the Multi-Head Attention and Feed Forward layers together.\n    Only after the output of each layer, you have to do the Layer Norm first and then the residual connection.\n    Sublayer is a lambda function\n    \"\"\"\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        # Returns the result after joining the Layer Norm and the residuals.\n        return x + self.dropout(sublayer(self.norm(x)))\n\n\ndef clones(module, N):\n    \"\"\"Clone model block, cloned model block parameters are not shared\"\"\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        The implementation of Feed Forward in Encoder and Deocder, which mainly contains a multilayer perceptron\n        args: \n        d_model：the input dimension of Encoder\n        d_ff：the intermediate dimension\n        dropout：the rate of dropout\n\n        \"\"\"\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.589882Z","iopub.execute_input":"2023-11-16T08:47:47.590188Z","iopub.status.idle":"2023-11-16T08:47:47.639857Z","shell.execute_reply.started":"2023-11-16T08:47:47.590155Z","shell.execute_reply":"2023-11-16T08:47:47.638838Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## Finish the implement of Encoder and EncoderLayer(10 marks)","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoderlayer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(encoderlayer, N)\n\n    def forward(self, x, mask):\n        \"\"\"\n        The implementation of Encoder, note that the core is a stack of N encoder(layers)\n        args：\n        encoderlayer：the implementation of encoder in Encoder\n        N：the number of encoder in Encoder, such as 6\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n    \n    \n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, multihead_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = multihead_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n\n\n    def forward(self, x, mask):\n        \"\"\"\n        The implementation of encoder in Encoder, which is made up of self-attention layer, feed forward and norm layer etc\n        args：\n        d_model：the input dimension of Encoder\n        multihead_attn：multihead attention module in encoder\n        feed_forward：the feed forward module in encoder\n        dropout：the rate of dropout\n        x：the input of Encoder\n        mask：the mask of multihead attention\n        \"\"\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.641207Z","iopub.execute_input":"2023-11-16T08:47:47.641480Z","iopub.status.idle":"2023-11-16T08:47:47.653582Z","shell.execute_reply.started":"2023-11-16T08:47:47.641455Z","shell.execute_reply":"2023-11-16T08:47:47.652566Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## Finish the implement of Decoder and DecoderLayer(10 marks)","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, decoderlayer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(decoderlayer, N)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"\"\"\n        The implementation of Decoder, the core is a stack of N decoder(layers)\n        args：\n        decoder layer：the implementation of decoder\n        N：the number of decoder in Decoder, such as 6\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return x\n    \n    \n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, multihead_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = multihead_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(d_model, dropout), 3)\n\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"\"\"\n        The implementation of a decoder in the Decoder, which is made up of self-attn, src_attn and feed forward\n        args：\n        d_model：the output dimension of Encoder\n        multihead_attn：the multihead attention module(self attention) in decoder\n        src_attn：the cross attention module in decoder\n        feed_forward：the feed forward module\n        memory：the output of Encoder\n        x：the input of Decoder\n        src_mask：the mask of cross attention module\n        tgt_mask：the mask of multihead attention module(self attention)\n        \"\"\"\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, x, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.655051Z","iopub.execute_input":"2023-11-16T08:47:47.655375Z","iopub.status.idle":"2023-11-16T08:47:47.668183Z","shell.execute_reply.started":"2023-11-16T08:47:47.655349Z","shell.execute_reply":"2023-11-16T08:47:47.667329Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"## Finish the implement of the whole model(10 marks)","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        \"\"\"\n        The implementation of Transformer.\n        args：\n        encoder: the encoder of the transformer\n        decoder: the decoder of the transformer\n        src_embed: the embedding of the source sentence\n        tgt_embed: the embedding of the target sentence\n        generator: the output of the final layer of the decoder\n        \"\"\"\n        super(Transformer, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def encode(self, src, src_mask):\n        \"\"\"\n        args: \n        src: the source sentence\n        src_mask: the masked source sentence \n        \"\"\"\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        \"\"\"\n        args: \n        memory: the output of encoder\n        src_mask: the masked source sentence\n        tgt: the target sentence\n        tgt_mask: the masked target sentence \n        \"\"\"\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"\"\"\n        args:\n        src: the source sentence\n        tgt: the target sentence\n        src_mask: the masked source sentence\n        tgt_mask: the masked target sentence \n        \"\"\"\n        memory = self.encode(src, src_mask)\n        out = self.decode(memory, src_mask, tgt, tgt_mask)\n        return self.generator(out)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.669400Z","iopub.execute_input":"2023-11-16T08:47:47.670187Z","iopub.status.idle":"2023-11-16T08:47:47.682377Z","shell.execute_reply.started":"2023-11-16T08:47:47.670150Z","shell.execute_reply":"2023-11-16T08:47:47.681468Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    # vocab: tgt_vocab\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        # perform the log_softmax operation (taking the logarithm of the softmax result).\n        return F.log_softmax(self.proj(x), dim=-1)\n\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n\n    attn = MultiHeadedAttention(h, d_model).to(DEVICE)\n\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n\n    position = PositionalEncoding(d_model, dropout).to(DEVICE)\n\n    model = Transformer(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE), c(position)),\n        Generator(d_model, tgt_vocab)).to(DEVICE)\n\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model.to(DEVICE)\n\n\ndef batch_greedy_decode(model, src, src_mask, max_len=64, start_symbol=2, end_symbol=3):\n    batch_size, src_seq_len = src.size()\n    results = [[] for _ in range(batch_size)]\n    stop_flag = [False for _ in range(batch_size)]\n    count = 0\n\n    memory = model.encode(src, src_mask)\n    tgt = torch.Tensor(batch_size, 1).fill_(start_symbol).type_as(src.data)\n\n    for s in range(max_len):\n        tgt_mask = subsequent_mask(tgt.size(1)).expand(batch_size, -1, -1).type_as(src.data)\n        out = model.decode(memory, src_mask, Variable(tgt), Variable(tgt_mask))\n\n        prob = model.generator(out[:, -1, :])\n        pred = torch.argmax(prob, dim=-1)\n\n        tgt = torch.cat((tgt, pred.unsqueeze(1)), dim=1)\n        pred = pred.cpu().numpy()\n        for i in range(batch_size):\n            # print(stop_flag[i])\n            if stop_flag[i] is False:\n                if pred[i] == end_symbol:\n                    count += 1\n                    stop_flag[i] = True\n                else:\n                    results[i].append(pred[i].item())\n            if count == batch_size:\n                break\n\n    return results\n\n\ndef greedy_decode(model, src, src_mask, max_len=64, start_symbol=2, end_symbol=3):\n    memory = model.encode(src, src_mask)\n    # Initialize the prediction content as a 1×1 tensor, fill it with the ID of the start symbol ('BOS'), and set the type to the input data type (LongTensor).\n    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n    # Iterate over the length subscript of the output\n    for i in range(max_len - 1):\n        # decode obtains the hidden layer representation\n        out = model.decode(memory,\n                           src_mask,\n                           Variable(ys),\n                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n        # convert the hidden representation into a log-softmax probability distribution over the words in the dictionary.\n        prob = model.generator(out[:, -1])\n        # obtain the predicted word ID with the highest probability at the current position.\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]\n        if next_word == end_symbol:\n            break\n        # concatenate the predicted character ID at the current position with the previously predicted content.\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n    return ys","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.683792Z","iopub.execute_input":"2023-11-16T08:47:47.684092Z","iopub.status.idle":"2023-11-16T08:47:47.704053Z","shell.execute_reply.started":"2023-11-16T08:47:47.684042Z","shell.execute_reply":"2023-11-16T08:47:47.703128Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"## Beam search\nBeam search is a search algorithm commonly used in natural language processing and machine translation to find the most likely sequence of words given a set of possible choices. It works by exploring a set of candidate solutions and gradually narrowing down the options by selecting only the most promising ones based on a certain scoring function. This approach is particularly useful in cases where the search space is large and exhaustive search is not feasible. ","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass Beam:\n    \"\"\" Beam search \"\"\"\n\n    def __init__(self, size, pad, bos, eos, device=False):\n\n        self.size = size\n        self._done = False\n        self.PAD = pad\n        self.BOS = bos\n        self.EOS = eos\n        # The score for each translation on the beam.\n        self.scores = torch.zeros((size,), dtype=torch.float, device=device)\n        self.all_scores = []\n\n        # The backpointers at each time-step.\n        self.prev_ks = []\n\n        # The outputs at each time-step.\n        # Initialize to [BOS, PAD, PAD ..., PAD]\n        self.next_ys = [torch.full((size,), self.PAD, dtype=torch.long, device=device)]\n        self.next_ys[0][0] = self.BOS\n\n    def get_current_state(self):\n        \"\"\"Get the outputs for the current timestep.\"\"\"\n        return self.get_tentative_hypothesis()\n\n    def get_current_origin(self):\n        \"\"\"Get the backpointers for the current timestep.\"\"\"\n        return self.prev_ks[-1]\n\n    @property\n    def done(self):\n        return self._done\n\n    def advance(self, word_logprob):\n        \"\"\"Update beam status and check if finished or not.\"\"\"\n        num_words = word_logprob.size(1)\n\n        # Sum the previous scores.\n        if len(self.prev_ks) > 0:\n            beam_lk = word_logprob + self.scores.unsqueeze(1).expand_as(word_logprob)\n        else:\n            # in initial case,\n            beam_lk = word_logprob[0]\n\n        flat_beam_lk = beam_lk.view(-1)\n        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True)\n\n        self.all_scores.append(self.scores)\n        self.scores = best_scores\n\n        # bestScoresId is flattened as a (beam x word) array,\n        # so we need to calculate which word and beam each score came from\n        prev_k = best_scores_id // num_words\n        self.prev_ks.append(prev_k)\n        self.next_ys.append(best_scores_id - prev_k * num_words)\n\n        # End condition is when top-of-beam is EOS.\n        if self.next_ys[-1][0].item() == self.EOS:\n            self._done = True\n            self.all_scores.append(self.scores)\n\n        return self._done\n\n    def sort_scores(self):\n        \"\"\"Sort the scores.\"\"\"\n        return torch.sort(self.scores, 0, True)\n\n    def get_the_best_score_and_idx(self):\n        \"\"\"Get the score of the best in the beam.\"\"\"\n        scores, ids = self.sort_scores()\n        return scores[1], ids[1]\n\n    def get_tentative_hypothesis(self):\n        \"\"\"Get the decoded sequence for the current timestep.\"\"\"\n\n        if len(self.next_ys) == 1:\n            dec_seq = self.next_ys[0].unsqueeze(1)\n        else:\n            _, keys = self.sort_scores()\n            hyps = [self.get_hypothesis(k) for k in keys]\n            hyps = [[self.BOS] + h for h in hyps]\n            dec_seq = torch.LongTensor(hyps)\n\n        return dec_seq\n\n    def get_hypothesis(self, k):\n        \"\"\" Walk back to construct the full hypothesis. \"\"\"\n        # print(k.type())\n        hyp = []\n        for j in range(len(self.prev_ks) - 1, -1, -1):\n            hyp.append(self.next_ys[j + 1][k])\n            k = self.prev_ks[j][k]\n\n        return list(map(lambda x: x.item(), hyp[::-1]))\n\n\ndef beam_search(model, src, src_mask, max_len, pad, bos, eos, beam_size, device):\n    \"\"\" Translation work in one batch \"\"\"\n\n    def get_inst_idx_to_tensor_position_map(inst_idx_list):\n        \"\"\" Indicate the position of an instance in a tensor. \"\"\"\n        return {inst_idx: tensor_position for tensor_position, inst_idx in enumerate(inst_idx_list)}\n\n    def collect_active_part(beamed_tensor, curr_active_inst_idx, n_prev_active_inst, n_bm):\n        \"\"\" Collect tensor parts associated to active instances. \"\"\"\n\n        _, *d_hs = beamed_tensor.size()\n        n_curr_active_inst = len(curr_active_inst_idx)\n        # active instances (elements of batch) * beam search size x seq_len x h_dimension\n        new_shape = (n_curr_active_inst * n_bm, *d_hs)\n\n        # select only parts of tensor which are still active\n        beamed_tensor = beamed_tensor.view(n_prev_active_inst, -1)\n        beamed_tensor = beamed_tensor.index_select(0, curr_active_inst_idx)\n        beamed_tensor = beamed_tensor.view(*new_shape)\n\n        return beamed_tensor\n\n    def collate_active_info(\n            src_enc, src_mask, inst_idx_to_position_map, active_inst_idx_list):\n        # Sentences which are still active are collected,\n        # so the decoder will not run on completed sentences.\n        n_prev_active_inst = len(inst_idx_to_position_map)\n        active_inst_idx = [inst_idx_to_position_map[k] for k in active_inst_idx_list]\n        active_inst_idx = torch.LongTensor(active_inst_idx).to(device)\n\n        active_src_enc = collect_active_part(src_enc, active_inst_idx, n_prev_active_inst, beam_size)\n        active_inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n        active_src_mask = collect_active_part(src_mask, active_inst_idx, n_prev_active_inst, beam_size)\n\n        return active_src_enc, active_src_mask, active_inst_idx_to_position_map\n\n    def beam_decode_step(\n            inst_dec_beams, len_dec_seq, enc_output, inst_idx_to_position_map, n_bm):\n        \"\"\" Decode and update beam status, and then return active beam idx \"\"\"\n\n        def prepare_beam_dec_seq(inst_dec_beams, len_dec_seq):\n            dec_partial_seq = [b.get_current_state() for b in inst_dec_beams if not b.done]\n            # Batch size x Beam size x Dec Seq Len\n            dec_partial_seq = torch.stack(dec_partial_seq).to(device)\n            # Batch size*Beam size x Dec Seq Len\n            dec_partial_seq = dec_partial_seq.view(-1, len_dec_seq)\n            return dec_partial_seq\n\n        def predict_word(dec_seq, enc_output, n_active_inst, n_bm):\n            assert enc_output.shape[0] == dec_seq.shape[0] == src_mask.shape[0]\n            out = model.decode(enc_output, src_mask,\n                               dec_seq,\n                               subsequent_mask(dec_seq.size(1))\n                               .type_as(src.data))\n            word_logprob = model.generator(out[:, -1])\n            word_logprob = word_logprob.view(n_active_inst, n_bm, -1)\n\n            return word_logprob\n\n        def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):\n            active_inst_idx_list = []\n            for inst_idx, inst_position in inst_idx_to_position_map.items():\n                is_inst_complete = inst_beams[inst_idx].advance(\n                    word_prob[inst_position])  # Fill Beam object with assigned probabilities\n                if not is_inst_complete:  # if top beam ended with eos, we do not add it\n                    active_inst_idx_list += [inst_idx]\n\n            return active_inst_idx_list\n\n        n_active_inst = len(inst_idx_to_position_map)\n\n        # get decoding sequence for each beam\n        # size: Batch size*Beam size x Dec Seq Len\n        dec_seq = prepare_beam_dec_seq(inst_dec_beams, len_dec_seq)\n\n        # get word probabilities for each beam\n        # size: Batch size x Beam size x Vocabulary\n        word_logprob = predict_word(dec_seq, enc_output, n_active_inst, n_bm)\n\n        # Update the beam with predicted word prob information and collect incomplete instances\n        active_inst_idx_list = collect_active_inst_idx_list(\n            inst_dec_beams, word_logprob, inst_idx_to_position_map)\n\n        return active_inst_idx_list\n\n    def collect_hypothesis_and_scores(inst_dec_beams, n_best):\n        all_hyp, all_scores = [], []\n        for inst_idx in range(len(inst_dec_beams)):\n            scores, tail_idxs = inst_dec_beams[inst_idx].sort_scores()\n            all_scores += [scores[:n_best]]\n\n            hyps = [inst_dec_beams[inst_idx].get_hypothesis(i) for i in tail_idxs[:n_best]]\n            all_hyp += [hyps]\n        return all_hyp, all_scores\n\n    with torch.no_grad():\n        # -- Encode\n        src_enc = model.encode(src, src_mask)\n\n        #  Repeat data for beam search\n        NBEST = beam_size\n        batch_size, sent_len, h_dim = src_enc.size()\n        src_enc = src_enc.repeat(1, beam_size, 1).view(batch_size * beam_size, sent_len, h_dim)\n        src_mask = src_mask.repeat(1, beam_size, 1).view(batch_size * beam_size, 1, src_mask.shape[-1])\n\n        # -- Prepare beams\n        inst_dec_beams = [Beam(beam_size, pad, bos, eos, device) for _ in range(batch_size)]\n\n        # -- Bookkeeping for active or not\n        active_inst_idx_list = list(range(batch_size))\n        inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n\n        # -- Decode\n        for len_dec_seq in range(1, max_len + 1):\n\n            active_inst_idx_list = beam_decode_step(\n                inst_dec_beams, len_dec_seq, src_enc, inst_idx_to_position_map, beam_size)\n\n            if not active_inst_idx_list:\n                break  # all instances have finished their path to <EOS>\n            # filter out inactive tensor parts (for already decoded sequences)\n            src_enc, src_mask, inst_idx_to_position_map = collate_active_info(\n                src_enc, src_mask, inst_idx_to_position_map, active_inst_idx_list)\n\n    batch_hyp, batch_scores = collect_hypothesis_and_scores(inst_dec_beams, NBEST)\n\n    return batch_hyp, batch_scores\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.705325Z","iopub.execute_input":"2023-11-16T08:47:47.705661Z","iopub.status.idle":"2023-11-16T08:47:47.742559Z","shell.execute_reply.started":"2023-11-16T08:47:47.705629Z","shell.execute_reply":"2023-11-16T08:47:47.741550Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## BLEU score\nBLEU (Bilingual Evaluation Understudy) score is a metric commonly used to evaluate the quality of machine-translated text by comparing it to one or more reference translations. It measures the overlap between the machine-generated text and the reference translations, taking into account the precision of n-grams (sequences of n words) and the brevity penalty. The higher the BLEU score, the better the translation\nquality.","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:47:47.743667Z","iopub.execute_input":"2023-11-16T08:47:47.743972Z","iopub.status.idle":"2023-11-16T08:48:01.475223Z","shell.execute_reply.started":"2023-11-16T08:47:47.743942Z","shell.execute_reply":"2023-11-16T08:48:01.473868Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/df/c0/ff53cb76c1b050ad25d056877ba6d3f6fa964134370c4ccf57ad933d6f72/sacrebleu-2.3.2-py3-none-any.whl.metadata\n  Downloading sacrebleu-2.3.2-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m426.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.8.8)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.24.3)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nDownloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.autograd import Variable\n\nimport logging\nimport sacrebleu\nfrom tqdm import tqdm\n\n\n\ndef run_epoch(data, model, loss_compute):\n    total_tokens = 0.\n    total_loss = 0.\n\n    for batch in tqdm(data):\n        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n\n        total_loss += loss\n        total_tokens += batch.ntokens\n    return total_loss / total_tokens\n\n\ndef train(train_data, dev_data, model, model_par, criterion, optimizer):\n    \"\"\"train and save model\"\"\"\n    best_bleu_score = 0.0\n    bleu_score_list = []\n    dev_loss_list = []\n    early_stop = config.early_stop\n    for epoch in range(1, config.epoch_num + 1):\n        # train the model\n        model.train()\n        train_loss = run_epoch(train_data, model_par,\n                               MultiGPULossCompute(model.generator, criterion, config.device_id, optimizer))\n        logging.info(\"Epoch: {}, loss: {}\".format(epoch, train_loss))\n        # model validation\n        model.eval()\n        dev_loss = run_epoch(dev_data, model_par,\n                             MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n        bleu_score = evaluate(dev_data, model)\n        \n        dev_loss_list.append(dev_loss.cpu().detach().numpy())\n        bleu_score_list.append(bleu_score)\n        \n        logging.info('Epoch: {}, Dev loss: {}, Bleu Score: {}'.format(epoch, dev_loss, bleu_score))\n\n        # save the current model if its loss on the dev set for the current epoch is better than the previously recorded best loss, and update the best loss value.\n        if bleu_score > best_bleu_score:\n            torch.save(model.state_dict(), config.output_model_path)\n            best_bleu_score = bleu_score\n            early_stop = config.early_stop\n            logging.info(\"-------- Save Best Model! --------\")\n        else:\n            early_stop -= 1\n            logging.info(\"Early Stop Left: {}\".format(early_stop))\n        if early_stop == 0:\n            logging.info(\"-------- Early Stop! --------\")\n            break\n    return dev_loss_list, bleu_score_list\n\n\nclass LossCompute:\n    \"\"\"A single-gpu loss compute and train function.\"\"\"\n\n    def __init__(self, generator, criterion, opt=None):\n        self.generator = generator\n        self.criterion = criterion\n        self.opt = opt\n\n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n                              y.contiguous().view(-1)) / norm\n        loss.backward()\n        if self.opt is not None:\n            self.opt.step()\n            if config.use_noamopt:\n                self.opt.optimizer.zero_grad()\n            else:\n                self.opt.zero_grad()\n        return loss.data.item() * norm.float()\n\nclass MultiGPULossCompute:\n    \"\"\"A multi-gpu loss compute and train function.\"\"\"\n\n    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n        # Send out to different gpus.\n        self.generator = generator\n        self.criterion = nn.parallel.replicate(criterion, devices=devices)\n        self.opt = opt\n        self.devices = devices\n        self.chunk_size = chunk_size\n\n    def __call__(self, out, targets, normalize):\n        total = 0.0\n        generator = nn.parallel.replicate(self.generator, devices=self.devices)\n        out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)\n        out_grad = [[] for _ in out_scatter]\n        targets = nn.parallel.scatter(targets, target_gpus=self.devices)\n\n        # Divide generating into chunks.\n        chunk_size = self.chunk_size\n        for i in range(0, out_scatter[0].size(1), chunk_size):\n            # Predict distributions\n            out_column = [[Variable(o[:, i:i + chunk_size].data,\n                                    requires_grad=self.opt is not None)]\n                          for o in out_scatter]\n            gen = nn.parallel.parallel_apply(generator, out_column)\n\n            # Compute loss.\n            y = [(g.contiguous().view(-1, g.size(-1)),\n                  t[:, i:i + chunk_size].contiguous().view(-1))\n                 for g, t in zip(gen, targets)]\n            loss = nn.parallel.parallel_apply(self.criterion, y)\n\n            # Sum and normalize loss\n            l_ = nn.parallel.gather(loss, target_device=self.devices[0])\n            l_ = l_.sum() / normalize\n            total += l_.data\n\n            # Backprop loss to output of transformer\n            if self.opt is not None:\n                l_.backward()\n                for j, l in enumerate(loss):\n                    out_grad[j].append(out_column[j][0].grad.data.clone())\n\n        # Backprop all loss through transformer.\n        if self.opt is not None:\n            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n            o1 = out\n            o2 = nn.parallel.gather(out_grad,\n                                    target_device=self.devices[0])\n            o1.backward(gradient=o2)\n            self.opt.step()\n            if config.use_noamopt:\n                self.opt.optimizer.zero_grad()\n            else:\n                self.opt.zero_grad()\n        return total * normalize\n\n    \ndef evaluate(data, model, mode='eval', use_beam=True):\n    \"\"\"Predict using the trained model and print the output\"\"\"\n    sp_chn = chinese_tokenizer_load()\n    engs = []\n    trg = []\n    res = []\n    with torch.no_grad():\n        for batch in tqdm(data):\n            en_sent = batch.src_text\n            cn_sent = batch.trg_text\n            src = batch.src\n            src_mask = (src != 0).unsqueeze(-2)\n            if use_beam:\n                decode_result, _ = beam_search(model, src, src_mask, config.max_len,\n                                               config.padding_idx, config.bos_idx, config.eos_idx,\n                                               config.beam_size, device)\n            else:\n                decode_result = batch_greedy_decode(model, src, src_mask,\n                                                    max_len=config.max_len)\n            decode_result = [h[0] for h in decode_result]\n            translation = [sp_chn.decode_ids(_s) for _s in decode_result]\n            trg.extend(cn_sent)\n            res.extend(translation)\n            engs.extend(en_sent)\n    if mode == 'test':\n        for i in range(len(trg)):\n            line = \"idx: \\n \" + str(i) +': \\n' + engs[i] +'\\n label: '+trg[i] + '\\n predict:' + res[i] + '\\n'\n            print(line)     \n    trg = [trg]\n    bleu = sacrebleu.corpus_bleu(res, trg, tokenize='zh')\n    return float(bleu.score)\n\n\ndef test(data, model, criterion, mode='eval'):\n    with torch.no_grad():\n        # load model\n        model.load_state_dict(torch.load(config.output_model_path))\n        model_par = torch.nn.DataParallel(model)\n        model.eval()\n        # predict\n        test_loss = run_epoch(data, model_par,\n                              MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n        bleu_score = evaluate(data, model, mode)\n        logging.info('Test loss: {},  Bleu Score: {}'.format(test_loss, bleu_score))\n    return test_loss.cpu().detach().numpy(), bleu_score\n\n\ndef translate(src, model, model_path,  use_beam=True):\n    \"\"\"Predict a single sentence using the trained model and print the output.\"\"\"\n    sp_chn = chinese_tokenizer_load()\n    with torch.no_grad():\n        model.load_state_dict(torch.load(model_path))\n        model.eval()\n        src_mask = (src != 0).unsqueeze(-2)\n        if use_beam:\n            decode_result, _ = beam_search(model, src, src_mask, config.max_len,\n                                           config.padding_idx, config.bos_idx, config.eos_idx,\n                                           config.beam_size, device)\n            decode_result = [h[0] for h in decode_result]\n        else:\n            decode_result = batch_greedy_decode(model, src, src_mask, max_len=config.max_len)\n        translation = [sp_chn.decode_ids(_s) for _s in decode_result]\n        return translation[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:01.477580Z","iopub.execute_input":"2023-11-16T08:48:01.478037Z","iopub.status.idle":"2023-11-16T08:48:01.620483Z","shell.execute_reply.started":"2023-11-16T08:48:01.477996Z","shell.execute_reply":"2023-11-16T08:48:01.619695Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import logging\nimport numpy as np\n\nimport torch\n# In order to run this cell in kaggle, I add this statement.\nfrom torch.utils.data import DataLoader \n\nclass NoamOpt:\n    \"\"\"Optim wrapper that implements rate.\"\"\"\n\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n\n    def step(self):\n        \"\"\"Update parameters and rate\"\"\"\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n\n    def rate(self, step=None):\n        \"\"\"Implement `lrate` above\"\"\"\n        if step is None:\n            step = self._step\n        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n\n\ndef get_std_opt(model):\n    \"\"\"for batch_size 32, 5530 steps for one epoch, 2 epoch for warm-up\"\"\"\n    return NoamOpt(model.src_embed[0].d_model, 1, 10000,\n                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n\ndef run():\n    set_logger(config.log_path)\n\n    train_dataset = MTDataset(config.train_data_path)\n    dev_dataset = MTDataset(config.dev_data_path)\n    test_dataset = MTDataset(config.test_data_path)\n\n    logging.info(\"-------- Dataset Build! --------\")\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size,\n                                  collate_fn=train_dataset.collate_fn)\n    dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=config.batch_size,\n                                collate_fn=dev_dataset.collate_fn)\n    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size,\n                                 collate_fn=test_dataset.collate_fn)\n\n    logging.info(\"-------- Get Dataloader! --------\")\n    # initialize the model\n    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n    model_par = torch.nn.DataParallel(model)\n    # train the model\n    if config.use_smoothing:\n        criterion = LabelSmoothing(size=config.tgt_vocab_size, padding_idx=config.padding_idx, smoothing=0.1)\n        criterion.cuda()\n    else:\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n    if config.use_noamopt:\n        optimizer = get_std_opt(model)\n    else:\n        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n    train_loss_list, train_bleu_score_list= train(train_dataloader, dev_dataloader, model, model_par, criterion, optimizer)\n    test_loss, test_bleu_score = test(test_dataloader, model, criterion)\n    \n    return train_loss_list, train_bleu_score_list, test_loss, test_bleu_score\n\n\ndef check_opt():\n    \"\"\"check learning rate changes\"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n    opt = get_std_opt(model)\n    # Three settings of the lrate hyperparameters.\n    opts = [opt,\n            NoamOpt(512, 1, 20000, None),\n            NoamOpt(256, 1, 10000, None)]\n    plt.plot(np.arange(1, 50000), [[opt.rate(i) for opt in opts] for i in range(1, 50000)])\n    plt.legend([\"512:10000\", \"512:20000\", \"256:10000\"])\n    plt.show()\n\n    \ndef one_sentence_translate(sent, model_path,beam_search=True):\n    # model initialation\n    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n    BOS = english_tokenizer_load().bos_id()  # 2\n    EOS = english_tokenizer_load().eos_id()  # 3\n    src_tokens = [[BOS] + english_tokenizer_load().EncodeAsIds(sent) + [EOS]]\n    batch_input = torch.LongTensor(np.array(src_tokens)).to(device)\n    return translate(batch_input, model, model_path, use_beam=beam_search)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:01.622257Z","iopub.execute_input":"2023-11-16T08:48:01.623028Z","iopub.status.idle":"2023-11-16T08:48:01.645782Z","shell.execute_reply.started":"2023-11-16T08:48:01.622988Z","shell.execute_reply":"2023-11-16T08:48:01.644838Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"train_loss_list, train_bleu_score_list, test_loss, test_bleu_score = run()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:01.647327Z","iopub.execute_input":"2023-11-16T08:48:01.647645Z","iopub.status.idle":"2023-11-16T08:48:13.092853Z","shell.execute_reply.started":"2023-11-16T08:48:01.647618Z","shell.execute_reply":"2023-11-16T08:48:13.091334Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stderr","text":"  0%|          | 0/1580 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([30, 8, 127, 64])\ntorch.Size([30, 8, 127, 64])\ntorch.Size([30, 8, 127, 64])\ntorch.Size([30, 1, 127, 127])\ntorch.Size([30, 8, 127, 127])\n==================================================================\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1580 [00:04<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 8, 128, 64])\ntorch.Size([34, 1, 1, 128])\ntorch.Size([34, 8, 128, 128])\n==================================================================\n==================================================================\ntorch.Size([30, 8, 127, 64])\ntorch.Size([30, 8, 127, 64])\ntorch.Size([30, 8, 127, 64])\ntorch.Size([30, 1, 127, 127])\ntorch.Size([30, 8, 127, 127])\n==================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss_list, train_bleu_score_list, test_loss, test_bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[69], line 70\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr)\n\u001b[0;32m---> 70\u001b[0m train_loss_list, train_bleu_score_list\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_par\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m test_loss, test_bleu_score \u001b[38;5;241m=\u001b[39m test(test_dataloader, model, criterion)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss_list, train_bleu_score_list, test_loss, test_bleu_score\n","Cell \u001b[0;32mIn[68], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data, dev_data, model, model_par, criterion, optimizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mepoch_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 32\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_par\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mMultiGPULossCompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, train_loss))\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# model validation\u001b[39;00m\n","Cell \u001b[0;32mIn[68], line 15\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(data, model, loss_compute)\u001b[0m\n\u001b[1;32m     12\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data):\n\u001b[0;32m---> 15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_compute(out, batch\u001b[38;5;241m.\u001b[39mtrg_y, batch\u001b[38;5;241m.\u001b[39mntokens)\n\u001b[1;32m     18\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/656968424.py\", line 46, in forward\n    out = self.decode(memory, src_mask, tgt, tgt_mask)\n  File \"/tmp/ipykernel_47/656968424.py\", line 35, in decode\n    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1672127285.py\", line 14, in forward\n    x = layer(x, memory, src_mask, tgt_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1672127285.py\", line 44, in forward\n    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, x, src_mask))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1002088172.py\", line 228, in forward\n    return x + self.dropout(sublayer(self.norm(x)))\n  File \"/tmp/ipykernel_47/1672127285.py\", line 44, in <lambda>\n    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, x, src_mask))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1002088172.py\", line 184, in forward\n    key = key.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\nRuntimeError: shape '[30, -1, 8, 64]' is invalid for input of size 2228224\n"],"ename":"RuntimeError","evalue":"Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/656968424.py\", line 46, in forward\n    out = self.decode(memory, src_mask, tgt, tgt_mask)\n  File \"/tmp/ipykernel_47/656968424.py\", line 35, in decode\n    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1672127285.py\", line 14, in forward\n    x = layer(x, memory, src_mask, tgt_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1672127285.py\", line 44, in forward\n    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, x, src_mask))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1002088172.py\", line 228, in forward\n    return x + self.dropout(sublayer(self.norm(x)))\n  File \"/tmp/ipykernel_47/1672127285.py\", line 44, in <lambda>\n    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, x, src_mask))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_47/1002088172.py\", line 184, in forward\n    key = key.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\nRuntimeError: shape '[30, -1, 8, 64]' is invalid for input of size 2228224\n","output_type":"error"}]},{"cell_type":"code","source":"logging.info('Test loss: {},  Bleu Score: {}'.format(test_loss, test_bleu_score))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.093981Z","iopub.status.idle":"2023-11-16T08:48:13.094433Z","shell.execute_reply.started":"2023-11-16T08:48:13.094219Z","shell.execute_reply":"2023-11-16T08:48:13.094247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the training process","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs_list = list(range(len(train_loss_list)))\nplt.figure(figsize=(20, 8))\nplt.plot(epochs_list, train_loss_list)\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.095926Z","iopub.status.idle":"2023-11-16T08:48:13.096276Z","shell.execute_reply.started":"2023-11-16T08:48:13.096106Z","shell.execute_reply":"2023-11-16T08:48:13.096123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs_list = list(range(len(train_bleu_score_list)))\nplt.figure(figsize=(20, 8))\nplt.plot(epochs_list, train_bleu_score_list)\nplt.title('Model Bleu Score')\nplt.ylabel('Bleu Score')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.098450Z","iopub.status.idle":"2023-11-16T08:48:13.098826Z","shell.execute_reply.started":"2023-11-16T08:48:13.098657Z","shell.execute_reply":"2023-11-16T08:48:13.098674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test trained model","metadata":{}},{"cell_type":"code","source":"def translate_example(model_path=config.output_model_path):\n    \"\"\"单句翻译示例\"\"\" \n    sent = \"I love Xiamen University\"\n    tgt= \"我爱厦门大学\"\n    res = one_sentence_translate(sent, model_path,beam_search=False)\n    print(f'{sent} => {res} \\n label: {tgt} \\n')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.100210Z","iopub.status.idle":"2023-11-16T08:48:13.100550Z","shell.execute_reply.started":"2023-11-16T08:48:13.100384Z","shell.execute_reply":"2023-11-16T08:48:13.100401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate_example()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.102427Z","iopub.status.idle":"2023-11-16T08:48:13.102791Z","shell.execute_reply.started":"2023-11-16T08:48:13.102622Z","shell.execute_reply":"2023-11-16T08:48:13.102640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_dataset():\n    test_dataset = MTDataset(config.test_data_path)\n\n    logging.info(\"-------- Dataset Build! --------\")\n    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size,\n                                 collate_fn=test_dataset.collate_fn)\n\n    logging.info(\"-------- Get Dataloader! --------\")\n    # initialize the model\n    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n    model_par = torch.nn.DataParallel(model)\n    # train the model\n    if config.use_smoothing:\n        criterion = LabelSmoothing(size=config.tgt_vocab_size, padding_idx=config.padding_idx, smoothing=0.1)\n        criterion.cuda()\n    else:\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n    test_loss, test_bleu_score = test(test_dataloader, model, criterion, mode='test')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.104048Z","iopub.status.idle":"2023-11-16T08:48:13.104378Z","shell.execute_reply.started":"2023-11-16T08:48:13.104214Z","shell.execute_reply":"2023-11-16T08:48:13.104231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T08:48:13.105589Z","iopub.status.idle":"2023-11-16T08:48:13.105983Z","shell.execute_reply.started":"2023-11-16T08:48:13.105773Z","shell.execute_reply":"2023-11-16T08:48:13.105790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question(40 marks)\n1. Why can transformer train in parallel but not reference in parallel? (5 marks)\n\nAnswer:\n\n2. What is the relationship between the convolution operations and the attention operations? (10 marks)\n\nAnswer:\n\n3. Why is a mask needed after tokenization? Attention mechanisms also use masks, what are their functions respectively? (10 marks)\n\nAnswer:\n\n4. Why does Transformer introduce positional coding? Why  do RNN, GRU, LSTM not need to introduce positional coding? (5 marks)\n\nAnswer:\n\n5. After you finish your assignment, please describe the whole process of machine translation based transformer, in other word, how is an English sentence  translated into Chinese ? The more detailed, the better. (10 marks)\n\nAnswer:","metadata":{}},{"cell_type":"markdown","source":"## Last but not least\nWhen you finish this assignment, you got the understanding that the Transformer model consists of the Encoder module and the Decoder module, however the encoder-decoder models are one of the models in large languages models(LLM); the Encoder module and Decoder module can be used individually.\nWe would like to suggest you to read the following papers\n- Encoder-only: [BERT](https://aclanthology.org/N19-1423.pdf), [ViT](https://arxiv.org/pdf/2010.11929.pdf)\n- Decoder-pnly: [GPT-1/2/3/4](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), [ChatGPT](https://openai.com/research/gpt-4)\n- Encoder-Decoder: [T5](https://jmlr.org/papers/v21/20-074.html)\n\nFor more LLM, please refer to [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf) and [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}